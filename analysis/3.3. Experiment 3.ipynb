{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path().absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load training set\n",
    "training_set_df = pd.read_csv(path / \"assets/datasets/training-set-1.csv\", delimiter=\";\")\n",
    "\n",
    "# Get X and y from dataset\n",
    "X_train = list(training_set_df[\"texts\"])\n",
    "y_train = list(training_set_df[\"targets\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.text_cleaning import TextCleaning\n",
    "from pipeline.tokenize_mwt_pos_lemma import TokenizeMWTPOSLemma\n",
    "\n",
    "text_preprocessing_pipeline: Pipeline = Pipeline([\n",
    "    (\"text_cleaning\", TextCleaning()),\n",
    "    (\"tokenize_mwt_pos_lemma\", TokenizeMWTPOSLemma())\n",
    "])\n",
    "\n",
    "X_train = text_preprocessing_pipeline.transform(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.data.stopwords import STOPWORDS\n",
    "from pipeline.pos_filter import POSFilter\n",
    "from pipeline.stopword_removal import StopWordRemoval\n",
    "from pipeline.document_transformer import DocumentTransformer\n",
    "\n",
    "pos_filter_hyperparameters = {\n",
    "    \"pos\": (\"ADJ\",\"ADV\",\"NOUN\",\"PART\",\"VERB\")\n",
    "}\n",
    "\n",
    "stopword_removal_hyperparameters = {\n",
    "    \"stopwords\": STOPWORDS\n",
    "}\n",
    "\n",
    "document_transformer_hyperparameters = {\n",
    "    \"feat_attrs\": [\"lemma\",\"upos\"]\n",
    "}\n",
    "\n",
    "feature_selection_pipeline: Pipeline = Pipeline([\n",
    "    (\"pos_filter\", POSFilter(**pos_filter_hyperparameters, verbose=0)),\n",
    "    (\"stopword_removal\", StopWordRemoval(**stopword_removal_hyperparameters, verbose=0)),\n",
    "    (\"document_transformer\", DocumentTransformer(**document_transformer_hyperparameters, verbose=0))\n",
    "])\n",
    "\n",
    "X_train = feature_selection_pipeline.transform(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def fun(arg):\n",
    "    return arg\n",
    "\n",
    "tfidfvectorizer_hyperparameters = {\n",
    "    \"encoding\": \"ascii\",\n",
    "    \"decode_error\": \"ignore\",\n",
    "    \"strip_accents\": \"ascii\",\n",
    "    \"preprocessor\": fun,\n",
    "    \"tokenizer\": fun,\n",
    "    \"analyzer\": \"word\",\n",
    "    \"token_pattern\": None,\n",
    "    \"max_df\": 1.0\n",
    "}\n",
    "\n",
    "linearsvc_hyperparameters = {\n",
    "    \"loss\": \"squared_hinge\",\n",
    "    \"dual\": False,\n",
    "    \"multi_class\": \"ovr\",\n",
    "    \"max_iter\": 1000000,\n",
    "    \"random_state\": 42,\n",
    "    \"tol\": 0.0001,\n",
    "    \"fit_intercept\": True,\n",
    "}\n",
    "\n",
    "classification_pipeline: Pipeline = Pipeline([\n",
    "    (\"tfidfvectorizer\", TfidfVectorizer(**tfidfvectorizer_hyperparameters)),\n",
    "    (\"linearsvc\", LinearSVC(**linearsvc_hyperparameters))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import make_scorer, accuracy_score, matthews_corrcoef\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "# warnings.filterwarnings(\"default\", category=ConvergenceWarning)\n",
    "\n",
    "param_distributions = {\n",
    "    \"tfidfvectorizer__ngram_range\": ((1, 1), (1, 2), (1, 3)),\n",
    "    \"tfidfvectorizer__min_df\": (1, 3, 5, 10),\n",
    "    \"tfidfvectorizer__norm\": (\"l1\", \"l2\"),\n",
    "    \"tfidfvectorizer__sublinear_tf\": (True, False),\n",
    "    \"linearsvc__penalty\": (\"l1\", \"l2\"),\n",
    "    \"linearsvc__C\": (0.01, 0.1, 1),\n",
    "    \"linearsvc__intercept_scaling\": (0.1, 1.0, 10, 100),\n",
    "    \"linearsvc__class_weight\": (None, \"balanced\"),\n",
    "}\n",
    "\n",
    "n_iter = 200\n",
    "n_splits = 5\n",
    "train_size = 0.8\n",
    "n_jobs = 1\n",
    "verbose = 2\n",
    "\n",
    "randomized_search = RandomizedSearchCV(\n",
    "    estimator=classification_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=n_iter,\n",
    "    scoring=make_scorer(matthews_corrcoef),\n",
    "    n_jobs=n_jobs,\n",
    "    cv=StratifiedShuffleSplit(n_splits=n_splits, train_size=train_size, random_state=42),\n",
    "    verbose=verbose,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "randomized_search.fit(X_train, y_train)\n",
    "estimation = time.time() - t0\n",
    "\n",
    "cv_results_df = pd.DataFrame(randomized_search.cv_results_)\n",
    "cv_results_df = cv_results_df.rename(lambda col_name: col_name.split(\"__\")[-1] if \"param_\" in col_name else col_name, axis=\"columns\")\n",
    "\n",
    "col_names = [\n",
    "    \"ngram_range\",\"min_df\",\"norm\",\"sublinear_tf\",\n",
    "    \"penalty\",\"C\",\"intercept_scaling\",\"class_weight\",\n",
    "    \"split0_test_score\",\"split1_test_score\",\"split2_test_score\",\"split3_test_score\",\"split4_test_score\",\"mean_test_score\",\n",
    "    \"mean_fit_time\",\n",
    "    \"rank_test_score\"\n",
    "]\n",
    "\n",
    "cv_results_df = cv_results_df.reindex(columns=col_names)\n",
    "cv_results_df = cv_results_df.fillna(\"None\")\n",
    "cv_results_df.to_csv(path / \"assets/experiments/experiment_3_cv_results.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "print(f'Fitted {randomized_search.n_splits_} folds of {len(cv_results_df)} candidates, finished in {str(timedelta(seconds=estimation))}.')\n",
    "print(f\"Best score: {randomized_search.best_score_}\")\n",
    "print(\"Best hyper-parameters:\")\n",
    "randomized_search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6177d1207e3b4e003bd5f7e0d0e470f696ef2c8899bc099ae3ccb41c3d6c53a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
