{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path().absolute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(path / \"assets/dataset.csv\", delimiter=\";\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spacy-stanza model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c809f6374afe4a54b3d2a942bbacb151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 00:22:29 INFO: Downloading default packages for language: id (Indonesian) ...\n",
      "2023-02-05 00:22:29 INFO: File exists: C:\\Users\\novia\\stanza_resources\\id\\default.zip\n",
      "2023-02-05 00:22:31 INFO: Finished downloading models and saved to C:\\Users\\novia\\stanza_resources.\n",
      "2023-02-05 00:22:32 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd796ca0d93c42fc8e924df2b8526d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 00:22:32 INFO: Loading these models for language: id (Indonesian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-02-05 00:22:32 INFO: Use device: cpu\n",
      "2023-02-05 00:22:32 INFO: Loading: tokenize\n",
      "2023-02-05 00:22:32 INFO: Loading: mwt\n",
      "2023-02-05 00:22:32 INFO: Loading: pos\n",
      "2023-02-05 00:22:32 INFO: Loading: lemma\n",
      "2023-02-05 00:22:32 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# download indonesian model\n",
    "stanza.download(\"id\")\n",
    "\n",
    "import spacy_stanza\n",
    "\n",
    "# initialize pipeline\n",
    "nlp = spacy_stanza.load_pipeline(\"id\", processors=\"tokenize,mwt,pos,lemma\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test one sample\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text              saya tidak merasa terhina\n",
      "target_emotion                        sedih\n",
      "Name: 0, dtype: object\n",
      "\n",
      "+---+---------+---------+------+-----------------------------------------------+\n",
      "| I | Token   | Lemma   | POS  | Morph                                         |\n",
      "+---+---------+---------+------+-----------------------------------------------+\n",
      "| 0 | saya    | saya    | PRON | Number=Sing|Person=1|Polite=Form|PronType=Prs |\n",
      "| 1 | tidak   | tidak   | PART | Polarity=Neg                                  |\n",
      "| 2 | merasa  | rasa    | VERB | Mood=Ind|Voice=Act                            |\n",
      "| 3 | terhina | terhina | ADJ  |                                               |\n",
      "+---+---------+---------+------+-----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "row = df.loc[i]\n",
    "print(row, end=\"\\n\\n\")\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"I\", \"Token\", \"Lemma\", \"POS\", \"Morph\"]\n",
    "table.align = \"l\"\n",
    "\n",
    "for token in nlp(row[\"text\"]):\n",
    "    table.add_row([token.i, token.text, token.lemma_, token.pos_, str(token.morph)])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirty Text Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip text with its context\n",
    "texts_with_contexts = [\n",
    "    (\n",
    "        df.at[i, \"text\"],\n",
    "        {\n",
    "            \"id\": i,\n",
    "            \"target_emotion\": df.at[i, \"target_emotion\"]\n",
    "        }\n",
    "    )\n",
    "    for i in df.index\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17701 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# batch processing\n",
    "docs_with_contexts = tqdm(nlp.pipe(texts_with_contexts, as_tuples=True), total=len(texts_with_contexts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize doc context\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"id\", default=None, force=True)\n",
    "Doc.set_extension(\"target_emotion\", default=None, force=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the docs contexts\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 92/17701 [00:08<27:03, 10.84it/s] c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Due to multiword token expansion or an alignment issue, the original text has been replaced by space-separated expanded tokens.\n",
      "  doc = self._ensure_doc(doc_like)\n",
      "c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'melepaskan', 'permusuhan', 'yang', 'terhadap', 'siapa', 'pun', 'yang', 'saya', 'rasa', 'telah', 'mengani', 'aa', 'saya']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      "  2%|▏         | 339/17701 [00:33<29:21,  9.86it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'merasa', 'bermanfaat', 'untuk', 'mendokumemtasin', 'nya', 'untuk', 'orang-orang', 'yang', 'tidak', 'terbiasa', 'dengan', 'file', 'batch']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 17%|█▋        | 2993/17701 [04:59<26:49,  9.14it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['Saya', 'bisa', 'merasakan', 'ketidakakaban', 'nya', 'dan', 'saya', 'tidak', 'bisa', 'menghentikan', 'tubuh', 'saya', 'untuk', 'memberi', 'nya', 'respons', 'positif']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 25%|██▌       | 4507/17701 [07:36<22:21,  9.84it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'harus', 'terbuka', 'dengan', 'mereka', 'seperti', 'saya', 'dengan', 'beberapa', 'teman', 'saya', 'ketika', 'saya', 'merasa', 'bahwa', 'mereka', 'telah', 'mengani', 'aa', 'saya']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 28%|██▊       | 4888/17701 [08:14<23:27,  9.10it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'bisa', 'menghabiskan', 'hidup', 'saya', 'mengutuk', 'orang', 'lain', 'saya', 'merasa', 'telah', 'mengani', 'aa', 'orang', 'saya', 'atau', 'saya', 'namun', 'konsekuensi', 'saya', 'sendiri', 'aneh', 'nya', 'pahit']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 28%|██▊       | 4924/17701 [08:17<23:59,  8.88it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'pikir', 'saya', 'akhirnya', 'bisa', 'mengartikulusin', 'nya', 'prius', 'itu', 'dengan', 'sendiri', 'nya', 'yang', 'mengkilap', 'bahagia', 'al', 'gore', 'memakai', 'patagonia', 'dengan', 'cara', 'alaska', 'agak', 'berbahaya', 'karena', 'membuat', 'mengemudi', 'terasa', 'seperti', 'tindakan', 'yang', 'berbudi', 'luhur']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 28%|██▊       | 4975/17701 [08:23<19:00, 11.16it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'merasa', 'sedikit', 'kesepian', 'tentang', 'ketidakakadin', 'nya', 'sekarang']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 30%|███       | 5385/17701 [09:04<21:44,  9.44it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'berada', 'di', 'saat', 'gelap', 'dalam', 'hidup', 'saya', 'pada', 'saat', 'yang', 'tepat', 'sehingga', 'setiap', 'kali', 'saya', 'membaca', 'barang--baran', 'nya', ',', 'perasaan', 'empati', 'yang', 'sekilas', 'untuk', 'nya', 'dan', 'kemenangan', 'nya', 'dengan', 'cepat', 'digantikan', 'oleh', 'kepahitan', 'dan', 'kebencian', 'bersalah', 'terhadap', 'nya']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 33%|███▎      | 5907/17701 [10:01<17:26, 11.27it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'merasa', 'kasihan', 'pada', 'nya', 'dia', 'memiliki', 'hal', 'yang', 'baik', 'di', 'dh', 'tapi', 'dia', 'menyalahahgukan', 'nya', 'dan', 'dia', 'mengakibatkan', 'depresi', 'dan', 'diagnosis', 'kecemasan', 'umum', 'dengan', 'fitur', 'panik', 'dan', 'kemudian', 'kehilangan', 'nya']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 58%|█████▊    | 10260/17701 [17:24<11:17, 10.98it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'harap', 'Anda', 'yang', 'benar-benar', 'menemukan', 'ini', 'dan', 'membaca', 'nya', 'merasa', 'mungkin', 'terinspirasi', 'untuk', 'pergi', 'keluar', 'dan', 'membeli', 'beberapa', 'barang', 'ini', 'atau', 'bahkan', 'pergi', 'ke', 'gudang', 'dan', 'melihat', 'pakaian', 'apa', 'yang', 'disimpan', 'ibu', 'mu', 'dan', 'Anda', 'masih', 'memiliki', 'harapan', 'untuk', 'cocok', 'dan', 'campur', 'pakaian', 'Anda', 'untuk', 'musim', 'panas', 'ini', 'dan', 'bersenang-senen', 'lah']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 59%|█████▉    | 10464/17701 [17:44<11:19, 10.64it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['Saya', 'telah', 'merasa', 'adalah', 'semua', 'kesalahan', 'saya', 'bahwa', 'saya', 'telah', 'mengani', 'aa', 'dia', 'dan', 'menyebabkan', 'dia', 'meninggalkan', 'saya']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 65%|██████▍   | 11420/17701 [19:21<09:32, 10.97it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'merasa', 'terhina', 'karena', 'seorang', 'anak', 'laki-laki', 'harus', 'menuntun', 'saya', 'melewati', 'nya', 'karena', 'sakit', ',', 'saya', 'menghindari', 'tarian', 'melalui', 'semua', 'folkeskole', 'dan', 'saya', 'tidak', 'akan', 'menyia-nnyikan', 'nya']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 68%|██████▊   | 12010/17701 [20:22<09:21, 10.13it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'merasa', 'telah', 'mengani', 'aa', 'saya']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 77%|███████▋  | 13646/17701 [23:11<06:21, 10.63it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['Saya', 'pikir', 'bagian', 'perlindungan', 'adalah', 'bagian', 'di', 'mana', 'saya', 'merasa', 'beberapa', 'telah', 'menyalahahgukan', 'nya', 'lebih', 'dari', 'alasan', 'lain', 'yang', 'dimaksudkan', 'untuk', 'hak', 'memanggul', 'senjata']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 87%|████████▋ | 15443/17701 [26:53<05:32,  6.80it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'ingin', 'berteriak', 'untuk', 'meneriaki', 'semua', 'orang', 'yang', 'saya', 'rasa', 'telah', 'mengani', 'aa', 'saya', 'tapi', 'jujur', '\\u200b\\u200bapa', 'guna', 'nya', 'itu']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 91%|█████████ | 16125/17701 [28:19<03:06,  8.45it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['tangan', 'nya', 'mulai', 'membelai', 'penis', 'nya', 'yang', 'sakit', 'melalui', 'kain', 'celana', 'boxernya', 'dan', 'dia', 'secara', 'naluriah', 'melengkungkan', 'punggung', 'nya', 'untuk', 'merasakan', 'sensasi', 'yang', 'lebih', 'nikmat']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 91%|█████████ | 16148/17701 [28:22<02:48,  9.22it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'merasa', 'seperti', 'Anda', 'tidak', 'ada', 'tubuh', 'yang', 'saya', 'coba', 'sayangi', 'rasa', 'nya', 'seperti', 'saya', 'mengani', 'aa', 'orang', 'asing', 'yang', 'bahkan', 'tidak', 'saya', 'kenal']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      " 93%|█████████▎| 16544/17701 [29:10<01:44, 11.02it/s]c:\\Users\\novia\\anaconda3\\envs\\learning\\lib\\site-packages\\spacy\\language.py:1114: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saya', 'tidak', 'tahu', 'bagaimana', 'menghadapi', 'ini', 'saya', 'merasa', 'seperti', 'akan', 'berpisah', 'jika', 'saya', 'takut', 'saya', 'akan', 'mengasosisisin', 'nya', 'dengan', 'hal-hal', 'biasa', 'sehingga', 'saya', 'tidak', 'akan', 'pernah', 'melupakan', 'nya']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(doc_like)\n",
      "100%|██████████| 17701/17701 [31:41<00:00,  9.31it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for doc, contexts in docs_with_contexts:\n",
    "    for key, val in contexts.items():\n",
    "        doc._.set(key, val)\n",
    "\n",
    "    docs.append(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dirty Text Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "DocBin(\n",
    "    docs=docs,\n",
    "    store_user_data=True\n",
    ").to_disk(path / \"assets/docs.preprocessing.dirtyX`.spacy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a049b0bad07c8102381f075079858d81b559044f5f11852a1c1c499ec97cb65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
