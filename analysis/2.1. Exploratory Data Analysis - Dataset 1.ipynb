{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Dataset 1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path().absolute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stanza model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stanza\n",
    "\n",
    "# stanza.download(\"id\")\n",
    "\n",
    "# tokenizer = stanza.Pipeline(\"id\", processors=\"tokenize,mwt,pos,lemma\", use_gpu=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(path / \"assets/datasets/dataset-1.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[\"texts\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = tokenizer.bulk_process(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce docs\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, doc in enumerate(docs):\n",
    "#     tokens = []\n",
    "\n",
    "#     for sentence in doc.sentences:\n",
    "#         for token in sentence.tokens:\n",
    "#             tokens.extend(token.words)\n",
    "\n",
    "#     docs[i] = (doc.text, tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save tokenized docs\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path / \"assets/pickles/docs-1.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(docs, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenized docs\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path / \"assets/pickles/docs-1.pickle\", \"rb\") as f:\n",
    "    docs = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foreign word identification\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_words = set()\n",
    "\n",
    "# for text, tokens in tqdm(docs):\n",
    "#     for token in tokens:\n",
    "#         unique_words.add(token.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kbbi import KBBI, TidakDitemukan\n",
    "\n",
    "# foreign_words = set()\n",
    "\n",
    "# for word in tqdm(unique_words):\n",
    "#     try:\n",
    "#         KBBI(word)\n",
    "#     except TidakDitemukan:\n",
    "#         foreign_words.add(word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas = []\n",
    "\n",
    "# for text, tokens in tqdm(docs):\n",
    "#     for token in tokens:\n",
    "#         lemmas.append(token.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# Counter(lemmas).most_common()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech List\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pos = set()\n",
    "\n",
    "for text, tokens in docs:\n",
    "    for token in tokens:\n",
    "        if token.pos not in list_of_pos:\n",
    "            list_of_pos.add(token.pos)\n",
    "\n",
    "list_of_pos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common tokens of specific POS tag\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words=[]\n",
    "\n",
    "for text, tokens in docs:\n",
    "    for token in tokens:\n",
    "        if token.pos == \"X\":\n",
    "            words.append(token.text)\n",
    "\n",
    "counter=Counter(words).most_common()\n",
    "\n",
    "print(len(counter))\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2 = []\n",
    "\n",
    "for text, tokens in docs:\n",
    "    for token in tokens:\n",
    "        if token.text == \"love\":\n",
    "            print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6177d1207e3b4e003bd5f7e0d0e470f696ef2c8899bc099ae3ccb41c3d6c53a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
