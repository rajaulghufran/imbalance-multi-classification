{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path().absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\c10nGp4\\OneDrive\\Documents\\GitHub\\imbalance-multi-classification\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-06 14:53:17 INFO: DOWNLOAD STANZA MODEL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81baa96aae44cfc953671a5a814e481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.1.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 14:53:17 INFO: Downloading default packages for language: id (Indonesian) ...\n",
      "2023-06-06 14:53:18 INFO: File exists: C:\\Users\\c10nGp4\\stanza_resources\\id\\default.zip\n",
      "2023-06-06 14:53:20 INFO: Finished downloading models and saved to C:\\Users\\c10nGp4\\stanza_resources.\n",
      "2023-06-06 14:53:20 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-06 14:53:20 INFO: LOAD STANZA PIPELINE: tokenize,mwt,pos,lemma\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0118e946502b4aee83002b9f967aea30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130e5278b8cb4e85af9ae17fbe141af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-id/resolve/v1.5.0/models/pos/gsd.pt:   0%|          | 0.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 14:53:27 INFO: Loading these models for language: id (Indonesian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-06-06 14:53:27 INFO: Using device: cuda\n",
      "2023-06-06 14:53:27 INFO: Loading: tokenize\n",
      "2023-06-06 14:53:30 INFO: Loading: mwt\n",
      "2023-06-06 14:53:30 INFO: Loading: pos\n",
      "2023-06-06 14:53:30 INFO: Loading: lemma\n",
      "2023-06-06 14:53:30 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from pipeline.classification import Classification\n",
    "\n",
    "clf = Classification()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "training_set_df = pd.read_csv(path / \"assets/datasets/training-set-1.csv\", delimiter=\";\")\n",
    "# testing_set_df = pd.read_csv(path / \"assets/datasets/testing-set-1.csv\", delimiter=\";\")\n",
    "\n",
    "# # Select specific categories\n",
    "# # training_set_df = training_set_df[training_set_df[\"targets\"].isin([\"kaget\",\"takut\"])]\n",
    "# # testing_set_df = testing_set_df[testing_set_df[\"targets\"].isin([\"kaget\",\"takut\"])]\n",
    "\n",
    "# Get X and y from dataset\n",
    "X_train = list(training_set_df[\"texts\"])\n",
    "# y_train = list(training_set_df[\"targets\"])\n",
    "\n",
    "# X_test = list(testing_set_df[\"texts\"])\n",
    "# y_test = list(testing_set_df[\"targets\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-06 15:08:44 INFO: TEXT CLEANING\n",
      "2023-06-06 15:08:46 INFO: TOKENIZE, MWT, POS, LEMMA\n",
      "2023-06-06 15:09:43 INFO: TEXT CLEANING\n",
      "2023-06-06 15:09:43 INFO: TOKENIZE, MWT, POS, LEMMA\n",
      "2023-06-06 15:09:57 INFO: POS REMOVAL\n",
      "2023-06-06 15:09:57 INFO: STOPWORD REMOVAL\n",
      "2023-06-06 15:09:58 INFO: DOCUMENT TRANSFORMER\n",
      "2023-06-06 15:09:58 INFO: POS REMOVAL\n",
      "2023-06-06 15:09:58 INFO: STOPWORD REMOVAL\n",
      "2023-06-06 15:09:58 INFO: DOCUMENT TRANSFORMER\n"
     ]
    }
   ],
   "source": [
    "from pipeline.pos_filter import POS\n",
    "\n",
    "# Text Preprocessing\n",
    "X_train = clf.text_preprocessing_pipeline.transform(X_train)\n",
    "X_test = clf.text_preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "# Feature Selection\n",
    "\n",
    "# emotion classification\n",
    "clf.feature_selection_pipeline.named_steps[\"pos_filter\"].set_params(**{\"pos\": POS - set([\"ADP\",\"AUX\",\"CCONJ\",\"DET\",\"INTJ\",\"NUM\",\"PRON\",\"PROPN\",\"PUNCT\",\"SCONJ\",\"SYM\",\"X\"])})\n",
    "\n",
    "# aspect classification\n",
    "# clf.feature_selection_pipeline.named_steps[\"pos_filter\"].set_params(**{\"pos\": set([\"NOUN\",\"PROPN\"])})\n",
    "\n",
    "# extract word features\n",
    "clf.feature_selection_pipeline.named_steps[\"document_transformer\"].set_params(**{\"feat_attrs\": [\"text\"]})\n",
    "\n",
    "X_train = clf.feature_selection_pipeline.transform(X_train)\n",
    "X_test = clf.feature_selection_pipeline.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import timedelta\n",
    "\n",
    "# n_iter = 10\n",
    "# n_splits = 5\n",
    "# train_size = 0.8\n",
    "# n_jobs = 1\n",
    "# verbose = 3\n",
    "\n",
    "# param_distributions = {\n",
    "#     \"tfidfvectorizer__ngram_range\": ((1, 1), (1, 2), (2, 2)),\n",
    "#     \"tfidfvectorizer__min_df\": (0.01, 1, 3, 5, 10),\n",
    "#     \"tfidfvectorizer__max_df\": (0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "#     \"tfidfvectorizer__norm\": (None, \"l1\", \"l2\"),\n",
    "#     \"tfidfvectorizer__sublinear_tf\": (True, False),\n",
    "#     \"linearsvc__penalty\": (\"l1\",\"l2\"),\n",
    "#     \"linearsvc__loss\": (\"hinge\",\"squared_hinge\"),\n",
    "#     \"linearsvc__dual\": (True, False),\n",
    "#     \"linearsvc__tol\": (0.0001,),\n",
    "#     \"linearsvc__C\": (0.01, 0.1, 1, 10, 100),\n",
    "#     \"linearsvc__multi_class\": (\"ovr\",),\n",
    "#     \"linearsvc__fit_intercept\": (True, False),\n",
    "#     \"linearsvc__intercept_scaling\": (1.0,),\n",
    "#     \"linearsvc__class_weight\": (None, \"balanced\", {\"kaget\": 1, \"cinta\": 3, \"takut\": 4, \"marah\": 5, \"gembira\": 10, \"sedih\": 11}),\n",
    "#     \"linearsvc__max_iter\": (1000,)\n",
    "# }\n",
    "\n",
    "# randomized_search, estimation = clf.tuning(X_train, y_train, param_distributions, n_iter, n_splits, train_size, n_jobs, verbose)\n",
    "# cv_results_df = pd.DataFrame(randomized_search.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Fitted {randomized_search.n_splits_} folds of {len(cv_results_df)} candidates, finished in {str(timedelta(seconds=estimation))}.')\n",
    "# print(f\"Best score: {randomized_search.best_score_}\")\n",
    "# print(\"Best hyper-parameters:\")\n",
    "# randomized_search.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO Update classification pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.classification_pipeline = randomized_search.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR Re-train Model\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the best hyper-parameters\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.classification_pipeline.set_params(**randomized_search.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.train_preprocessed(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.to_disk(path / \"assets/models/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = path / \"assets/models/model.2023.05.24.14.40.25.798494.pickle\"\n",
    "# clf.from_disk(filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clf.test_preprocessed(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy, mcc = clf.score(y_test, y_pred)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"MCC:\", mcc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# normalize = None\n",
    "# # normalize=\"true\"\n",
    "\n",
    "# ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize=normalize, cmap=\"YlGn\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.classification_pipeline.named_steps[\"tfidfvectorizer\"].get_feature_names_out()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get random samples\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample([x for x in X_train if len(x) <= 50 and \"gugup\" in x], 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of a dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp = [\n",
    "    \"<b>Saya senang bisa melihat ðŸŽ† bersamamu</b>\",\n",
    "    \"Saya merasa benar-benar dirugikan dirinya!!!!\",\n",
    "    \"Video menyeramkan lainnya: https://www.youtube.com/shorts/ZIl2iiHy1oM. ***JANGAN DILIHAT KALAU TIDAK BERANI***\",\n",
    "    \"Saya merasa kesepian dan terisolasi akhir akhir ini\",\n",
    "    \"saya merasa sangat sedih saya berharap saya mati\",\n",
    "    \"saya merasa sedih hanya memikirkannya\",\n",
    "    \"saya merasa patah hati dan sedih\",\n",
    "    \"saya merasa sangat putus asa dan sedih\",\n",
    "    \"Saya selalu merasa tidak diinginkan dan sedih\",\n",
    "    \"saya merasa senang\",\n",
    "    \"saya merasa cukup senang minggu ini\",\n",
    "    \"saya sangat senang tapi saya merasa lelah\",\n",
    "    \"saya merasakan cinta dan perhatian sekarang\",\n",
    "    \"saya merasa disambut dan dicintai\",\n",
    "    \"saya merasa dicintai\",\n",
    "    \"saya merasa takut berada di dekat mereka\",\n",
    "    \"saya merasa sangat ketakutan dan gugup\",\n",
    "    \"saya merasa takut dan bodoh\",\n",
    "    \"saya merasa takut\",\n",
    "    \"saya sangat suka perasaan takut\",\n",
    "    \"saya merasa seperti orang bodoh\",\n",
    "    \"saya merasa bodoh tapi senang\",\n",
    "    \"saya merasa sangat bodoh karena jujur\",\n",
    "    \"saya merasa sangat bodoh dan malu\",\n",
    "    \"saya merasa bodoh dan tidak diinginkan\",\n",
    "    \"saya merasa kesal dan marah\",\n",
    "    \"saya berteriak dengan perasaan sangat marah\",\n",
    "    \"saya bahkan tidak bisa merasa marah karenanya\",\n",
    "    \"saya merasa sedikit kesal hari ini\",\n",
    "    \"saya bangun hari ini merasa kesal\",\n",
    "    \"saya merasa sangat gemetar dan sedih\",\n",
    "    \"saya menjadi bodoh merasa gugup\"\n",
    "]\n",
    "\n",
    "y_temp = [\n",
    "    \"gembira\",\n",
    "    \"marah\",\n",
    "    \"takut\",\n",
    "    \"sedih\",\n",
    "    \"sedih\",\n",
    "    \"sedih\",\n",
    "    \"sedih\",\n",
    "    \"sedih\",\n",
    "    \"sedih\",\n",
    "    \"gembira\",\n",
    "    \"gembira\",\n",
    "    \"gembira\",\n",
    "    \"cinta\",\n",
    "    \"cinta\",\n",
    "    \"cinta\",\n",
    "    \"takut\",\n",
    "    \"takut\",\n",
    "    \"takut\",\n",
    "    \"takut\",\n",
    "    \"takut\",\n",
    "    \"sedih\",\n",
    "    \"gembira\",\n",
    "    \"sedih\",\n",
    "    \"sedih\",\n",
    "    \"sedih\",\n",
    "    \"marah\",\n",
    "    \"marah\",\n",
    "    \"marah\",\n",
    "    \"marah\",\n",
    "    \"marah\",\n",
    "    \"sedih\",\n",
    "    \"sedih\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex.model_selection import train_test_split\n",
    "\n",
    "X_temp_train, X_temp_test, y_temp_train, y_temp_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "X_temp_train, X_temp_validation, y_temp_train, y_temp_validation = train_test_split(\n",
    "    X_temp_train,\n",
    "    y_temp_train,\n",
    "    test_size=0.21,\n",
    "    random_state=42,\n",
    "    stratify=y_temp_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_temp_train))\n",
    "\n",
    "# for x in X_temp_train:\n",
    "#     print(x)\n",
    "\n",
    "for y in y_temp_train:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_temp_validation))\n",
    "\n",
    "# for x in X_temp_validation:\n",
    "#     print(x)\n",
    "\n",
    "for y in y_temp_validation:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_temp_test))\n",
    "\n",
    "# for x in X_temp_test:\n",
    "#     print(x)\n",
    "\n",
    "for y in y_temp_test:\n",
    "    print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp_train_docs = clf.text_preprocessing_pipeline.transform(X_temp_train)\n",
    "X_temp_validation_docs = clf.text_preprocessing_pipeline.transform(X_temp_validation)\n",
    "X_temp_test_docs = clf.text_preprocessing_pipeline.transform(X_temp_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_selection_pipeline.named_steps[\"pos_filter\"].set_params(**{\"pos\": set([\"ADJ\",\"ADV\",\"NOUN\",\"PART\",\"VERB\"])})\n",
    "clf.feature_selection_pipeline.named_steps[\"document_transformer\"].set_params(**{\"feat_attrs\": [\"lemma\",\"upos\"]})\n",
    "X_temp_train_selected = clf.feature_selection_pipeline.transform(X_temp_train_docs)\n",
    "X_temp_validation_selected = clf.feature_selection_pipeline.transform(X_temp_validation_docs)\n",
    "X_temp_test_selected = clf.feature_selection_pipeline.transform(X_temp_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(X_temp_train_selected):\n",
    "    print(\", \".join(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(X_temp_validation_selected):\n",
    "    print(\", \".join(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(X_temp_test_selected):\n",
    "    print(\", \".join(doc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectorizer_hyperparameters = {\n",
    "    \"ngram_range\": (1, 1),\n",
    "    \"max_df\": (1.0),\n",
    "    \"min_df\": (1),\n",
    "    \"norm\": None,\n",
    "    \"sublinear_tf\": False\n",
    "}\n",
    "tfidfvectorizer = clf.classification_pipeline.named_steps[\"tfidfvectorizer\"]\n",
    "tfidfvectorizer.set_params(**tfidfvectorizer_hyperparameters)\n",
    "tfidfvectorizer.fit(X_temp_train_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, zip_ in enumerate(\n",
    "    zip(\n",
    "        dict(\n",
    "            sorted(\n",
    "                tfidfvectorizer.vocabulary_.items(),\n",
    "                key=lambda x: x[1]\n",
    "            )\n",
    "        ).items(),\n",
    "        tfidfvectorizer.idf_\n",
    "    )\n",
    "):\n",
    "    item, idf = zip_\n",
    "    k, v = item\n",
    "    df = 0\n",
    "\n",
    "    for doc in X_temp_train_selected:\n",
    "        if k in doc:\n",
    "            df+=1\n",
    "\n",
    "    print(i+1, k, df, round(idf, 5), sep=\"\\t\")\n",
    "    # print(i+1, k, df, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp_train_features = tfidfvectorizer.transform(X_temp_train_selected)\n",
    "X_temp_validation_features = tfidfvectorizer.transform(X_temp_validation_selected)\n",
    "X_temp_test_features = tfidfvectorizer.transform(X_temp_test_selected)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi classification\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearsvc_hyperparameters = {\n",
    "    \"penalty\": \"l2\",\n",
    "    \"loss\": \"squared_hinge\",\n",
    "    \"dual\": False,\n",
    "    \"tol\": 0.0001,\n",
    "    \"C\": 0.01,\n",
    "    \"multi_class\": \"ovr\",\n",
    "    \"fit_intercept\": True,\n",
    "    \"intercept_scaling\": 1.0,\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"max_iter\": 1000\n",
    "}\n",
    "linearsvc = clf.classification_pipeline.named_steps[\"linearsvc\"]\n",
    "linearsvc.set_params(**linearsvc_hyperparameters)\n",
    "linearsvc.fit(X_temp_train_features, y_temp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_temp_validation_pred = linearsvc.predict(X_temp_validation_features)\n",
    "y_temp_test_pred = linearsvc.predict(X_temp_test_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "print(accuracy_score(y_temp_validation, y_temp_validation_pred))\n",
    "print(matthews_corrcoef(y_temp_validation, y_temp_validation_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "print(accuracy_score(y_temp_test, y_temp_test_pred))\n",
    "print(matthews_corrcoef(y_temp_test, y_temp_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6177d1207e3b4e003bd5f7e0d0e470f696ef2c8899bc099ae3ccb41c3d6c53a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
